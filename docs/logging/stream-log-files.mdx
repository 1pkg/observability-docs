---
id: serverlessObservabilityStreamLogFiles
slug: /serverless/observability/stream-log-files
title: Stream log files
# description: Description to be written
tags: [ 'serverless', 'observability', 'how-to' ]
status: rough content
---

import RoughContent from '../partials/rough-content-notice.mdx'

<RoughContent />

{/* import MonitoringPrereqs from './transclusion/logs-metrics-get-started/monitoring-prereqs.mdx' */}
{/* import DownloadWidget from '../fleet/transclusion/tab-widgets/download-widget.mdx' */}
{/* import RunStandaloneWidget from '../fleet/transclusion/tab-widgets/run-standalone-widget.mdx' */}

<div id="logs-stream"></div>

In this guide, you'll learn how to take a log file from your host and send it to Elasticsearch using a standalone ((agent)). You'll configure the ((agent)) and your data streams using the Custom Logs integration. From there, you'll learn how to query your logs and use the data streams you've set up to have more control and flexibility when filtering your log data. 

<div id="logs-stream-prereq"></div>

## Prerequisites

<div style={{ backgroundColor: "#F1F4FA", width: "100%", height: "250px", textAlign: "center", paddingTop: "110px", marginBottom: "20px" }}><DocIcon type="documents" title="missing snippet"/> missing snippet</div>
{/* <MonitoringPrereqs  /> */}


<div id="logs-stream-install-config-agent"></div>

## Install and configure the standalone ((agent))

Install and configure the standalone ((agent)) to send your log data to ((es)) by completing the following steps:

1. Download and extract the ((agent)) installation package.
1. Configure the ((agent)) using the Custom Logs integration.
1. Install and start the ((agent)).

<div id="logs-stream-extract-agent"></div>

### Step 1: Download and extract the ((agent)) installation package

On your host, download and extract the installation package that corresponds with your system:

<div style={{ backgroundColor: "#F1F4FA", width: "100%", height: "250px", textAlign: "center", paddingTop: "110px", marginBottom: "20px" }}><DocIcon type="documents" title="missing snippet"/> missing snippet</div>
{/* <DownloadWidget  /> */}


<div id="logs-stream-agent-config"></div>

### Step 2: Configure the ((agent))

You can use the Custom Logs integration in ((kib)) to create and download an ((agent)) policy. 
Your agent policy sets the path to your log file and sets up your data streams.

Complete the following steps to create and download your ((agent)) policy:

1. Go to the ((kib)) home page and click **Add integrations**:

    <div style={{ backgroundColor: "#F1F4FA", width: "100%", height: "250px", textAlign: "center", paddingTop: "110px", marginBottom: "20px" }}><DocIcon type="image" title="missing image"/> missing image</div>
{/* ![((kib)) home page](images/logs-stream/-observability-add-integrations.png) */}

1. In the search bar, search for **custom** and select **Custom Logs**.
1. Click **Add Custom Logs** in the upper-right corner:

    <div style={{ backgroundColor: "#F1F4FA", width: "100%", height: "250px", textAlign: "center", paddingTop: "110px", marginBottom: "20px" }}><DocIcon type="image" title="missing image"/> missing image</div>
{/* ![add custom logs button location](images/logs-stream/-observability-add-custom-logs.png) */}

1. Click **Install ((agent))**.
1. Click **standalone mode**:

    <div style={{ backgroundColor: "#F1F4FA", width: "100%", height: "250px", textAlign: "center", paddingTop: "110px", marginBottom: "20px" }}><DocIcon type="image" title="missing image"/> missing image</div>
{/* ![link to running agents in standalone mode](images/logs-stream/-observability-standalone-link.png) */}

1. Add the path to your log file in the **Log file path** field.
1. Set the name for your dataset data stream in the **Dataset name** field. The dataset name describes the data ingested and its structure. This field can contain anything that signifies the source of the data. The default value is `generic`.
1. Click **Advanced options** to open the **Integration settings**.

    <div style={{ backgroundColor: "#F1F4FA", width: "100%", height: "250px", textAlign: "center", paddingTop: "110px", marginBottom: "20px" }}><DocIcon type="image" title="missing image"/> missing image</div>
{/* ![open integration settings](images/logs-stream/-observability-custom-log-advanced-options.png) */}

1. Set the name for your namespace data stream in the **Namespace** field. The namespace is useful for grouping data such as an environment (dev, prod, or qa), a team, or a strategic business unit. Using a namespace makes it easier to search for data from a given source by using a matching pattern. The default value is `default`.
1. Click **Save and continue**.
1. Click **Copy to clipboard**, and paste the configuration in the `elastic-agent.yml` file located on your host where you extracted the ((agent)) installation package.
1. We recommend using an API key to grant the agent access to ((es)). To create an API key for your agent, see [Create API keys for standalone agents](((fleet-guide))/grant-access-to-elasticsearch.html#create-api-key-standalone-agent).
1. Replace `username: '${ES_USERNAME}'` and `password: '${ES_PASSWORD}'` with `api_key:` in the `elastic-agent.yml` file, and add the API key you created in the previous step. For example:

    ```yaml
    [...]
    outputs:
    default:
    type: elasticsearch
    hosts:
    - 'https://da4e3a6298c14a6683e6064ebfve9ace.us-central1.gcp.cloud.es.io:443'
    api_key: _Nj4oH0aWZVGqM7MGop8:349p_U1ERHyIc4Nm8_AYkw
    [...]
    ```

<DocCallOut title="Note">
The format of the key is `<id>:<key>`. Make sure you selected **Beats** when you created your API key. Base64 encoded API keys are not currently supported in this configuration.
</DocCallOut>

<div id="logs-stream-install-agent"></div>

### Step 3: Install and start the ((agent))
With your configuration set, you're ready to install the ((agent)). From the agent directory, run the the command that corresponds with your system to install the ((agent)) as a service. Do **not** enroll the agent in Fleet.

<DocCallOut title="Note">
On macOS, Linux (tar package), and Windows, run the `install` command to
install ((agent)) as a managed service and start the service. The DEB and RPM
packages include a service unit for Linux systems with
systemd, so just enable then start the service.
</DocCallOut>

<div style={{ backgroundColor: "#F1F4FA", width: "100%", height: "250px", textAlign: "center", paddingTop: "110px", marginBottom: "20px" }}><DocIcon type="documents" title="missing snippet"/> missing snippet</div>
{/* <RunStandaloneWidget  /> */}


<div id="logs-stream-query-datastreams"></div>

## View and search your data

With your ((agent)) and data streams configured, you can now view, filter, and search your log data. In ((kib)), navigate to **Observability → Logs → Stream**, and use the search bar to search for your `data_stream.type`, `data_stream.dataset`, and `data_stream.namespace`. 

See the following examples for ways to search specific data types, datasets, or namespaces:

- `data_stream.type: logs` shows `logs` data streams.
- `data_stream.dataset: nginx.access` shows data streams with an `nginx.access` dataset.
- `data_stream.namespace: web-frontend` shows data streams with `web-frontend` namespace.

The following example shows the search results for logs with an `apm.error` dataset and a `default` namespace:

<div style={{ backgroundColor: "#F1F4FA", width: "100%", height: "250px", textAlign: "center", paddingTop: "110px", marginBottom: "20px" }}><DocIcon type="image" title="missing image"/> missing image</div>
{/* ![example search query on the logs stream page in ((kib))](images/logs-stream/-observability-stream-logs-example.png) */}

<div id="logs-stream-whats-next"></div>

## What's next?

For more information on deploying and managing logs in Elastic Observability, see the following links:

- The <DocLink id="serverlessObservabilityLogsResourceGuide">Logs resource guide</DocLink> consolidates links to documentation on sending log data, configuring logs, and analyzing logs.
- <DocLink id="serverlessObservabilityDiscoverAndExploreLogs">Log monitoring</DocLink> has information on visualizing and analyzing logs.