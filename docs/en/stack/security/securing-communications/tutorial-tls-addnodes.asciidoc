[role="xpack"]
[testenv="trial"]
[[encrypting-communications-hosts]]
=== Add nodes to your cluster

Up to this point, we have used a cluster with a single {es} node to get up and
running with the {stack}. An {es} _node_ is a single server that is part of your
cluster and stores pieces of your data called _shards_. 

image::security/images/elas_0202.png["A cluster with one node and three primary shards"]

You can add more nodes to your cluster and optionally designate specific purposes
for each node. For example, you can allocate master nodes, data nodes, ingest
nodes, machine learning nodes, and dedicated coordinating nodes. For details
about each node type, see {ref}/modules-node.html[Nodes].

In a single cluster, you can have as many nodes as you want but they must be
able to communicate with each other. The communication between nodes in a
cluster is handled by the {ref}/modules-transport.html[transport module]. To
secure your cluster, you must ensure that the internode communications are
encrypted.

NOTE: In this tutorial, we add more nodes by installing more copies of {es} on
the same machine. By default, {es} binds to loopback addresses for HTTP and
transport communication. This is fine for the purposes of this tutorial and for
downloading and playing with {es} in a test or development environment. When you
are deploying a production environment, however, you are generally adding nodes
on different machines so that your cluster is resilient to outages and avoids
data loss.  In a production scenario, there are additional requirements that are
not covered in this tutorial. See
{ref}/bootstrap-checks.html#_development_vs_production_mode[Development vs production mode].

Let's add two nodes to our cluster!

. Install two additional copies of {es}. Multiple nodes can share the same
directory. In this tutorial, however, let's keep things clear by putting each
copy in a separate folder. You can simply repeat the steps that you used to
install {es} in the 
{stack-gs}/get-started-elastic-stack.html#install-elasticsearch[Getting started with the {stack}]
tutorial.

. Verify that hostname resolution works in your cluster. For example, add the
following node names to your `/etc/hosts` file:
+
--
[source,sh]
----------------------------------
127.0.0.1 localhost node1.local node2.local node3.local
----------------------------------

IMPORTANT: We can modify `etc/hosts` for testing purposes, but in production you
must have a proper DNS set up.

--

. <<get-started-enable-security,Enable the {es} {security-features}>>. For
example, add the following setting to the `ES_PATH_CONF/elasticsearch.yml`
file for each node:
+
--
[source,yaml]
----
xpack.security.enabled: true
----
--

. Ensure that the nodes share the same `cluster.name` in their `elasticsearch.yml`
files. When you run multiple nodes on the same machine, each node automatically
discovers and joins the cluster as long as it has the same `cluster.name` as the
first node. For more information, see {ref}/cluster.name.html[cluster.name].

. Give each node a unique `node.name` in its `elasticsearch.yml` file. For
example, `node-1`, `node-2`, and `node-3`.

. If you want to use specific hostnames or port numbers for communication
between nodes in the cluster, add 
{ref}/modules-transport.html[TCP transport settings]. You can then provide a
list of these {ref}/modules-discovery-zen.html#discovery-seed-nodes[seed nodes],
which is used to discover the nodes in your cluster.
+
--
For example, add the following settings to the `ES_PATH_CONF/elasticsearch.yml`
file on the first node:

[source,yaml]
----
transport.host: node1.local
transport.tcp.port: 9301
discovery.zen.ping.unicast.hosts: ["node2.local:9302", "node3.local:9303"]
----

Add the following settings to the `ES_PATH_CONF/elasticsearch.yml` file on the
second node:

[source,yaml]
----
transport.host: node2.local
transport.tcp.port: 9302
discovery.zen.ping.unicast.hosts: ["node1.local:9301", "node3.local:9303"]
----

Add the following settings to the `ES_PATH_CONF/elasticsearch.yml` file on the
third node:

[source,yaml]
----
transport.host: node3.local
transport.tcp.port: 9303
discovery.zen.ping.unicast.hosts: ["node1.local:9301", "node2.local:9302"]
----
--

. Start each {es} node. For example, if you installed {es} with a `.tar.gz`
package, run the following command from each {es} directory:
+
--
["source","sh",subs="attributes,callouts"]
----------------------------------------------------------------------
./bin/elasticsearch
----------------------------------------------------------------------

See {ref}/starting-elasticsearch.html[Starting {es}].
--

. (Optional) Restart {kib}. For example, if you installed 
{kib} with a `.tar.gz` package, run the following command from the {kib} 
directory:
+
--
["source","sh",subs="attributes,callouts"]
----------------------------------------------------------------------
./bin/kibana
----------------------------------------------------------------------

See {kibana-ref}/start-stop.html[Starting and stopping {kib}]. 
--

. Specify the minimum number of master-eligible nodes that must be available to
form a cluster. 
+ 
--
By default, each node is eligible to be elected as the
{ref}/modules-node.html#master-node[master node] and control the cluster. To
avoid a _split brain_ scenario where multiple nodes elect themselves as the
master, set the `discovery.zen.minimum_master_nodes` on each node in the cluster.

For example, use the following API to change the
`discovery.zen.minimum_master_nodes` setting:

[source,js]
----------------------------------
PUT _cluster/settings
{
  "persistent": {
    "discovery.zen.minimum_master_nodes": 2
  }
}
----------------------------------
// CONSOLE 

NOTE: You can specify this setting in either the `elasticsearch.yml` on each 
node or across the cluster as a dynamic cluster setting. When {es} 
{security-features} are enabled, you must have `monitor` cluster privileges to 
view the cluster settings and `manage` cluster privileges to change them.

For more information, see
{ref}/cluster-update-settings.htmlp[Cluster update settings] and
{ref}/modules-node.html#split-brain[Avoiding split brain].
--

. Verify that your cluster now contains three nodes. For example, use the
{ref}/cluster-health.html[cluster health API]:
+
--
[source,js]
----------------------------------
GET _cluster/health
----------------------------------
// CONSOLE 

Confirm the `number_of_nodes` in the response from this API.
--

Now that you have multiple nodes, your data can be distributed across the
cluster in multiple primary and replica shards. For more information about the
concepts of clusters, nodes, and shards, see
{ref}/getting-started.html[Getting started with {es}].

image::security/images/elas_0204.png["A cluster with three nodes"]

